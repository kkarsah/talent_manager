# Update to your existing core/pipeline/enhanced_content_pipeline.py

# Add these imports at the top
from core.content.enhanced_scene_service import EnhancedSceneService
from core.content.video_stitching_service import VideoStitchingService

# Add these properties to your EnhancedContentPipeline class

@property
def scene_service(self):
    """Enhanced scene generation service"""
    if not hasattr(self, '_scene_service') or self._scene_service is None:
        self._scene_service = EnhancedSceneService()
    return self._scene_service

@property
def stitching_service(self):
    """Video stitching service"""
    if not hasattr(self, '_stitching_service') or self._stitching_service is None:
        self._stitching_service = VideoStitchingService()
    return self._stitching_service

# Replace the video creation section in create_enhanced_content with this:

async def _create_video_with_cogvideox_integration(
    self,
    script: str,
    audio_path: str,
    title: str,
    content_type: str,
    talent_name: str,
    use_cogvideox: Optional[bool] = None,
    force_static: bool = False
) -> Optional[str]:
    """Create video with CogVideoX integration"""
    
    try:
        logger.info(f"ðŸŽ¬ Creating video with enhanced pipeline")
        
        # Parse scenes from script
        scenes = self._parse_scenes_from_script(script)
        logger.info(f"ðŸ“‹ Parsed {len(scenes)} scenes from script")
        
        # Get audio duration for timing
        audio_duration = await self._get_audio_duration(audio_path)
        if not audio_duration:
            logger.warning("Could not determine audio duration, using default")
            audio_duration = 60.0  # Default fallback
        
        # Override strategy if forced
        if force_static:
            # Force all static by temporarily disabling CogVideoX
            original_cogvideox = self.scene_service.use_cogvideox
            self.scene_service.use_cogvideox = False
        
        # Generate scene content with intelligent routing
        scene_result = await self.scene_service.generate_scene_content(
            scenes=scenes,
            content_type=content_type,
            talent_name=talent_name,
            audio_duration=audio_duration
        )
        
        # Restore original setting if we modified it
        if force_static:
            self.scene_service.use_cogvideox = original_cogvideox
        
        if not scene_result['success']:
            logger.error(f"Scene generation failed: {scene_result.get('error')}")
            return await self._create_simple_fallback_video(audio_path, title, content_type)
        
        # Create final video filename
        method = scene_result['method']
        video_id = str(uuid.uuid4())[:8]
        output_filename = f"{content_type}_{method}_{video_id}.mp4"
        
        # Stitch segments with audio
        logger.info(f"ðŸ”§ Stitching {len(scene_result['segments'])} segments using {method}")
        
        final_video_path = await self.stitching_service.stitch_segments_with_audio(
            segments=scene_result['segments'],
            audio_path=audio_path,
            output_filename=output_filename,
            content_type=content_type
        )
        
        if final_video_path:
            logger.info(f"âœ… Enhanced video created: {final_video_path}")
            
            # Log method used for analytics
            self._log_video_method(method, len(scenes), content_type)
            
            return final_video_path
        else:
            logger.error("Video stitching failed, using fallback")
            return await self._create_simple_fallback_video(audio_path, title, content_type)
            
    except Exception as e:
        logger.error(f"Enhanced video creation failed: {e}")
        return await self._create_simple_fallback_video(audio_path, title, content_type)

def _parse_scenes_from_script(self, script: str) -> List[Dict[str, str]]:
    """Parse scenes from script with scene markers"""
    
    scenes = []
    lines = script.split('\n')
    current_scene = None
    
    for line in lines:
        line = line.strip()
        
        # Look for scene markers like [Scene: description] or [Opening: description]
        if line.startswith('[') and ']:' in line:
            if current_scene:
                scenes.append(current_scene)
            
            # Extract scene description
            scene_desc = line[1:line.find(']:')]
            current_scene = {
                'description': scene_desc,
                'content': ''
            }
        elif current_scene and line and not line.startswith('['):
            # Add content to current scene
            current_scene['content'] += line + ' '
    
    # Add final scene
    if current_scene:
        scenes.append(current_scene)
    
    # If no explicit scenes found, create default scenes based on content
    if not scenes:
        # Split into logical sections for scene generation
        content_parts = script.split('\n\n')  # Split by paragraphs
        
        if len(content_parts) >= 3:
            scenes = [
                {'description': 'Professional tech studio with Alex CodeMaster', 'content': content_parts[0]},
                {'description': 'Code editor and development workspace', 'content': content_parts[1]},
                {'description': 'Live demonstration and examples', 'content': content_parts[2] if len(content_parts) > 2 else ''}
            ]
        else:
            scenes = [
                {'description': 'Professional educational environment', 'content': script}
            ]
    
    return scenes

def _log_video_method(self, method: str, scene_count: int, content_type: str):
    """Log video creation method for analytics"""
    logger.info(f"ðŸ“Š Video Method Analytics: {method} | Scenes: {scene_count} | Type: {content_type}")

async def _get_audio_duration(self, audio_path: str) -> Optional[float]:
    """Get audio duration using ffprobe"""
    try:
        import subprocess
        
        cmd = [
            'ffprobe', '-v', 'quiet',
            '-show_entries', 'format=duration',
            '-of', 'csv=p=0',
            audio_path
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        
        if result.returncode == 0:
            return float(result.stdout.strip())
        else:
            logger.error(f"ffprobe failed: {result.stderr}")
            return None
            
    except Exception as e:
        logger.error(f"Audio duration detection failed: {e}")
        return None

# Update the main create_enhanced_content method to use the new video creation:

# Replace this section in your create_enhanced_content method:
# OLD: video_path = await self.video_creator.create_video(...)
# NEW:

# Add method tracking to the result:
# Update your result dictionary to include:
result = {
    "success": True,
    "job_id": job_id,
    "title": generated_content.title,
    "description": generated_content.description,
    "tags": generated_content.tags,
    "video_path": video_path,
    "audio_path": audio_path,
    "video_creation_method": self._determine_method_used(video_path),  # NEW
    "scene_count": len(self._parse_scenes_from_script(generated_content.script)),  # NEW
    "capabilities": self.scene_service.get_capabilities(),  # NEW
    "upload_result": upload_result,
    "enhanced": True,
    "duration": await self._get_video_duration(video_path) if video_path else 0,
    "timestamp": datetime.now().isoformat()
}

def _determine_method_used(self, video_path: str) -> str:
    """Determine method used from video filename"""
    if not video_path:
        return "unknown"
    
    path_str = str(video_path)
    if "_cogvideox_" in path_str:
        return "cogvideox"
    elif "_hybrid_" in path_str:
        return "hybrid"
    elif "_static_" in path_str:
        return "static"
    else:
        return "fallback"

# Add capability checking method:
def get_enhanced_capabilities(self) -> Dict[str, Any]:
    """Get enhanced pipeline capabilities"""
    return {
        "scene_service": self.scene_service.get_capabilities(),
        "stitching_service": self.stitching_service.get_stitching_capabilities(),
        "recommended_usage": {
            "short_content": "Use CogVideoX for maximum engagement",
            "long_content": "Use hybrid approach for key scenes",
            "promotional": "Use CogVideoX for professional appeal"
        }
    }